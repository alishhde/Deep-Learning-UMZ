{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "The Backpropagation algorithm is a supervised learning method for multilayer feed-forward networks from the field of Artificial Neural Networks.\n",
    "\n",
    "Feed-forward neural networks are inspired by the information processing of one or more neural cells, called a neuron. A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body. The axon carries the signal out to synapses, which are the connections of a cell’s axon to other cell’s dendrites.\n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. The system is trained using a supervised learning method, where the error between the system’s output and a known expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this tutorial.\n",
    "\n",
    "In classification problems, best results are achieved when the network has one neuron in the output layer for each class value. For example, a 2-class or binary classification problem with the class values of A and B. These expected outputs would have to be transformed into binary vectors with one column for each class value. Such as [1, 0] and [0, 1] for A and B respectively. This is called a one hot encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Network\n",
    "\n",
    "Let’s start with something easy, the creation of a new network ready for training.\n",
    "\n",
    "Each neuron has a set of weights that need to be maintained. One weight for each input connection and an additional weight for the bias. We will need to store additional properties for a neuron during training, therefore we will use a dictionary to represent each neuron and store properties by names such as __weights__ for the weights.\n",
    "\n",
    "A network is organized into layers. The input layer is really just a row from our training dataset. The first real layer is the hidden layer. This is followed by the output layer that has one neuron for each class value.\n",
    "\n",
    "We will organize layers as arrays of dictionaries and treat the whole network as an array of layers.\n",
    "\n",
    "It is good practice to initialize the network weights to small random numbers. In this case, will we use uniform random numbers in the range of -1 to 1.\n",
    "\n",
    "In the program the __initialize_networks()__ method is a function that creates a new neural network ready for training. It accepts three parameters, the number of inputs, the number of neurons to have in the hidden layer and the number of outputs, and returns the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagate\n",
    "\n",
    "We can calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values.\n",
    "\n",
    "We call this forward-propagation.\n",
    "\n",
    "It is the technique we will need to generate predictions during training that will need to be corrected, and it is the method we will need after the network is trained to make predictions on new data.\n",
    "\n",
    "We can break forward propagation down into three parts:\n",
    "\n",
    "1. Neuron Activation\n",
    "2. Function Activation\n",
    "3. Forward Propagation\n",
    "\n",
    "\n",
    "#### Neuron Activation\n",
    "The first step is to calculate the activation of one neuron given an input.\n",
    "\n",
    "__activation = sum(weights * inputs) + bias__\n",
    "\n",
    "As the question has not talk about the bias, we assign the bias to the 0 by default. we have caluculated the activation in the __activate__ method which takes the weights(which is our network weights) and the inputs(which we will get it from the data in this case), when we call it. Notice that activation here is the output of the input layer or the input of the hidden layer. \n",
    "\n",
    "\n",
    "#### Function Activation\n",
    "Our activation method here is Sigmoid. We send the activation to this method and it returns, the activation. We do this to see what the neuron output actually is. In the following you see the Sigmoid function: __sigmoid = 1 / (1 + e^(-activation))__\n",
    "\n",
    "#### Forward Propagation\n",
    "Forward propagating an input is straightforward.\n",
    "\n",
    "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer.\n",
    "\n",
    "Below is a function named forward_propagate() that implements the forward propagation for a row of data from our dataset with our neural network.\n",
    "\n",
    "You can see that a neuron’s output value is stored in the neuron with the name ‘output‘. You can also see that we collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the number of epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373ef95405d844d79d8df71edd65ef86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d065ccceadef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/alishhde/Computer Engineering/Teachers/Dr  N.Maghsoudi/Computational Intelligence/HW/HW3/Q1/MLP.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;31m#Let's Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesire_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;31m#Let's draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/alishhde/Computer Engineering/Teachers/Dr  N.Maghsoudi/Computational Intelligence/HW/HW3/Q1/MLP.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, desire_output, epochs)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# This counter is gonna determine the input and output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We're in Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "\"\"\"                                                                  IN THE NAME OF GOD                                                                              \"\"\"\n",
    "\"\"\"                                                                 ALI SHOHADAALHOSSEINI                                                                            \"\"\"\n",
    "\"\"\"                                                           A STUDENT AT UNVERSITY OF MAZANDARAN                                                                   \"\"\"\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import MLP\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "class MLP:\n",
    "    \"\"\" \n",
    "        Multilayer perceptron:\n",
    "        Step 1: Initialize the weights and bias with small-randomized values,\n",
    "        Step 2: Propagate all values in the input layer until the output layer (Forward Propagation),\n",
    "            Step 2.1: Calculate sum of the input * weights for each neurons entry\n",
    "            Step 2.2: ..\n",
    "            Step 2.3: ..\n",
    "            Step 2.4: ..\n",
    "        Step 3: Update weight and bias in the inner layers(Backpropagation),\n",
    "        Step 4: Do it until that the stop criterion(sometimes stop criterion is epoch) is satisfied. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.input_size = 4      # input dimension\n",
    "        self.hidden_size = 4     # hiddeen dimension\n",
    "        self.output_size = 3     # output dimension\n",
    "        self.alpha = alpha       # learning rate\n",
    "        self.hist = {'loss':[], 'acc':[]}\n",
    "        self.sigmoid_output = 0\n",
    "\n",
    "    def initialize_networks(self):\n",
    "        \"\"\" \n",
    "        As we have said in the jupyter file, this method returns us a network.\n",
    "        input_size: is the number of input neurons\n",
    "        hidden_size: is the number of hidden neurons\n",
    "        output_size: is the number of output neurons \n",
    "        at the end, network will be returned. OUR net\n",
    "        work looks like the below. we have 4 input in \n",
    "        this question and each of them has 4 weights,\n",
    "        we only show the first neurons weights and also \n",
    "        we have 4 and 3 hidden and output layer.\n",
    "        so what we expect from the returned network is \n",
    "        a list of 7 (4 input + 3 output) weights which\n",
    "        consist of 4 weight itself, kind of 7*4 matrices.\n",
    "\n",
    "       NEURON   INPUT_LAYER              HIDDEN_LAYER             OUTPUT_LAYER\n",
    "                  ----                        ----\n",
    "         #1      |    | ===================> |    |  ======\n",
    "                  ----  ** -- +++++           ----   ** ++ ====        ----\n",
    "                          ** --   ++++                 ** ++   =====> |    |\n",
    "                  ----      ** --    +++      ----       ** ++         ----\n",
    "         #2      |    |       **  --    +++> |    |        ** ++\n",
    "                  ----          ** ---        ----           ** ++     ----\n",
    "                                 **  ---                      **  ++> |    |\n",
    "                  ----            *    --     ----             **      ----\n",
    "         #3      |    |           *      --> |    |             **\n",
    "                  ----            **          ----               **    ----\n",
    "                                    **                            **> |    |\n",
    "                  ----                **      ----                     ----\n",
    "         #4      |    |                 ***> |    |\n",
    "                  ----                        ----   \n",
    "        \"\"\"        \n",
    "        network = list()\n",
    "        self.hidden_layer = [ [np.random.uniform(-1, 1) for i in range(self.hidden_size)] for i in range(self.input_size)]\n",
    "        network.append(self.hidden_layer)\n",
    "        self.output_layer = [ [np.random.uniform(-1, 1) for i in range(self.output_size)] for i in range(self.hidden_size)]\n",
    "        network.append(self.output_layer)\n",
    "        \"\"\"\n",
    "            Network is a list consist of two list where each list consist of list of \n",
    "            each neuron where each neuron is a another list consist of its weight \n",
    "        \"\"\"\n",
    "        return network\n",
    "        \n",
    "    def sigmoid(self, activation):\n",
    "        \"\"\" This is our activation function for the\n",
    "        hidden layer which is sigmoid function  \n",
    "        in this question. Activation here is a list of \n",
    "        the dot product of the layer, it's actually is a list.\n",
    "        \"\"\"\n",
    "        next_layer_input_list = [0 for i in range(len(activation))]\n",
    "        cons_L = 1.0\n",
    "        cons_K = 1\n",
    "        # Since the output of the activation function is next layer input so,\n",
    "        # we name our variable next_layer_input_list\n",
    "        for i in range(len(activation)):\n",
    "            next_layer_input_list[i] += (cons_L)/(1.0 + (np.exp(-(cons_K * activation[i]))))\n",
    "        return next_layer_input_list\n",
    "    \n",
    "    def sigmoid_deriv(self):\n",
    "        \"\"\" It takes x as input and and its \n",
    "        derivation is: x * (1 - x)\n",
    "        x : is actually the output of the sigmoig function\n",
    "        \"\"\"\n",
    "        out = [0 for i in range(len(self.for_sigmoid_deriv))]\n",
    "        for i in range(len(self.for_sigmoid_deriv)):\n",
    "            out[i] += self.for_sigmoid_deriv[i] * ( 1 - self.for_sigmoid_deriv[i] )\n",
    "        return out\n",
    "\n",
    "    def softmax(self, activation):\n",
    "        \"\"\" This is our activation function for the\n",
    "        output layer which is softmax function  \n",
    "        in this question \n",
    "        This function returns a list of each neuron outputs\n",
    "        \"\"\"\n",
    "        tmp_output = [0 for i in range(len(activation))]\n",
    "        sum = 0\n",
    "        # Calculating e^number\n",
    "        for i in range(len(activation)):\n",
    "            tmp_output[i] += np.exp(activation[i])\n",
    "        # calculating the sum of the numbers\n",
    "        for _ in tmp_output:\n",
    "            sum += _\n",
    "        # calculating each neuron output in output layer\n",
    "        for i in range(len(activation)):\n",
    "            tmp_output[i] = tmp_output[i] / sum\n",
    "        return tmp_output\n",
    "\n",
    "    def forward(self, inputs, network, bias=1, bias_bol=False):\n",
    "        layer_counter = 0\n",
    "        which_activation_function = ['sigmoid', 'softmax']\n",
    "        for layer in network:\n",
    "            input_counter = 0 # This counter, counts the input number, to defie which inputs time it is\n",
    "            list_of_sum_of_each_neuron = [0 for i in range((len(layer[layer_counter])))]\n",
    "            \n",
    "            for w_neuron in layer:\n",
    "                \"\"\" For each loop in this << for loop >> we're gonna calculate the input of the\n",
    "                Hidden layer's neuron, which then must go into the activation function for the \n",
    "                Hidden layer and at last it will be the input of the hidden layer. Each w_neuron\n",
    "                is the weights of the next layer neurons. Here in this question we have below\n",
    "                w_neuron[0] == [w1, w2, w3, w4],\n",
    "                w_neuron[1] == [w1, w2, w3, w4],\n",
    "                w_neuron[2] == [w1, w2, w3, w4],\n",
    "                w_neuron[3] == [w1, w2, w3, w4],\n",
    "                w_neuron's index defines each neurons in the input layer we have, and length of each\n",
    "                w_neuron is the number of neuron we have in the next layer, So as you must understood \n",
    "                EACH weight in EACH w_neuron is the weight of EACH neuron in the next layer.\n",
    "                So ... \"\"\"\n",
    "                sumPlaceInListOfSum_counter = 0\n",
    "                for each_weight in w_neuron:\n",
    "                    mult_inp_and_weigh = inputs[input_counter] * each_weight # In the first loop it's w_neuron[0][0] * inputs_of_first_layer[0] \n",
    "                    list_of_sum_of_each_neuron[sumPlaceInListOfSum_counter] += mult_inp_and_weigh\n",
    "                    sumPlaceInListOfSum_counter += 1\n",
    "                input_counter += 1 #defining next input because we are going for the next neuron\n",
    "                \n",
    "            # Then we check if we have bias, we then add the bias to each list_of_sum_of_each_neuron\n",
    "            if bias_bol:\n",
    "                # adding bias \n",
    "                for i in range(len(list_of_sum_of_each_neuron)):\n",
    "                    list_of_sum_of_each_neuron[i] += bias\n",
    "\n",
    "            # Let's review how continue will work\n",
    "            \"\"\" Till here the sum of weights * input for each neuron of next layer has been computed,\n",
    "            Now we must send this list of number which is << list_of_sum_of_each_neuron >> to the \n",
    "            activation function of the next layer which in this question for the hidden layer is \n",
    "            the sigmoid function and for the output layer it's softmax function. So we are sending \n",
    "            this list which is the sum of input * weights for each neuron of next layer to calculate \n",
    "            this activation function which will give us the input of next layer and we can use that to\n",
    "            do this loop again for other layers in continue. So\n",
    "                Step 1: Send The << list_of_sum_of_each_neuron >> to the activation function of next layer\n",
    "                Step 2: Get the input of each neuron from the current activation function which will be return by it\n",
    "                Step 3: Loop Again and do this for the next layer. \n",
    "            \"\"\"\n",
    "            # In our problem this condition will only execute when we are in hidden layer\n",
    "            if which_activation_function[layer_counter] == 'sigmoid':\n",
    "                \n",
    "                # Step 1: call sigmoid and get the input of next layer or the output of current layer\n",
    "                outputs_of_layer = self.sigmoid(list_of_sum_of_each_neuron)\n",
    "                self.for_sigmoid_deriv = deepcopy(outputs_of_layer)\n",
    "                self.sigmoid_output = deepcopy(outputs_of_layer)\n",
    "\n",
    "                # Step 2\n",
    "                inputs = outputs_of_layer ## New inputs for the next layer initialize here\n",
    "                # print(\"Calculating the input of next layer with the sigmoid function END.... \\n\")\n",
    "                # print(\"The outputs_of_layer of the current layer which also is the input of next layer if we have other layer: \", outputs_of_layer,\"\\n\")\n",
    "                layer_counter += 1\n",
    "                \n",
    "                # Step 3 loop\n",
    "\n",
    "            # In our problem this condition will only execute when we are in output layer\n",
    "            elif which_activation_function[layer_counter] == 'softmax':\n",
    "                \n",
    "                # Step 1: call softmax and get the input of next layer or the output of current layer\n",
    "                outputs_of_layer = self.softmax(list_of_sum_of_each_neuron) # output of network produce here\n",
    "                \n",
    "                # Step 2\n",
    "                inputs = outputs_of_layer ## New inputs for the next layer initialize here\n",
    "                layer_counter += 1\n",
    "                \n",
    "                # Step 3 loop\n",
    "\n",
    "        \"\"\" Now our forward propagation has been finished, In the Next step we are going to compare this \n",
    "        output produced by our network, with the main output which the problem has gave us in the question \"\"\"\n",
    "        \n",
    "        \"\"\" Return of this function is a vector of output of our network, this outout has been created by softmax\n",
    "         and now its time to use the cross entropy function \"\"\"\n",
    "        return outputs_of_layer\n",
    "\n",
    "    def cross_entropy(self, desire_output, our_output):\n",
    "        \"\"\" Cross entropy is our loss function.\n",
    "        In this Example we are gonna use cross entropy \n",
    "        instead of MSE \"\"\"\n",
    "        # Defining desire outputs vector\n",
    "        if desire_output == 0:       # neuron one's class is activated\n",
    "            main_output = [1, 0, 0]\n",
    "        elif desire_output == 1:     # neuron two's class is activated\n",
    "            main_output = [0, 1, 0]\n",
    "        elif desire_output == 2:     # neuron three's class is activated\n",
    "            main_output = [0, 0, 1]\n",
    "            \n",
    "        loss = 0\n",
    "        for i in range(len(main_output)):\n",
    "            loss += main_output[i] * np.log(our_output[i])\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, input_of_epoch, desire_output, our_output, network):\n",
    "        # this is realy hard to understand,\n",
    "        \"\"\" In this method we're gonna implement backpropagation.\n",
    "        out_input: is actually the output of the Sigmoid function. As \n",
    "        you know the output of the activation function of the \n",
    "        hidden-layer is the input of the output-layer.\n",
    "\n",
    "        Step 1:\n",
    "            Now we are at the end of the network and want to go back,\n",
    "            we must calculate Dcost/Dw , we can calculate that by \n",
    "            calculating Dcost/Dz * Dz/Dw where z is the the output of \n",
    "            our network that has been returned by the feed-forward and\n",
    "            we even can simple this equation much more, we can write this instead \n",
    "            Dcost_Dw_out = ( our_output - desire_output ) * out_input\n",
    "\n",
    "            ..... So in the first step we have calculated the gradian of the \n",
    "            output-layer, and till here we are ready to update the weights of\n",
    "            the output-layer, But we don’t update the weights of the output-layer,\n",
    "            first we must calculate the gradian of the hidden-layer, then we'll\n",
    "            go for updating the weights of both layers.\n",
    "\n",
    "        Step 2:\n",
    "            Here we want to calculate the gradian of the hidden-layer which is \n",
    "            Dcost/Dw_hid and from chain rules we know that its equal to the \n",
    "            Dcost/Dah where ah is the output of the Sigmoid activation function \n",
    "            in the hidden layer * dah_dzh where zh is the weighted input of the \n",
    "            hidden-layer's activation function, I mean the one value which enters\n",
    "            into the sigmoid activation function * dzh_dwh where wh is the hidden-layer's \n",
    "            weight. so:\n",
    "            Dcost_Dah = compare * output_weight(network[2][each neuron])\n",
    "            dah_dzh = sigmoide_deriv(zh)\n",
    "            dzh_dwh = main input of that epoch\n",
    "            Dcost_Dwh[i] += Dcost_Dah[i] * dah_dzh[i] * dzh_dwh[i]\n",
    "\n",
    "        Step 3:\n",
    "            at last step we want to calculate the new weights and update them\n",
    "            to reach the best numbers in output.\n",
    "            Step 3.1: hidden-layer weights\n",
    "            W_new_hidden = W_old + self.alpha * Dcost_Dwh\n",
    "            Step 3.1: output-layer weights\n",
    "            W_new_hidden = W_old + self.alpha * Dcost_Dw_out\n",
    "\n",
    "            \"\"\"\n",
    "        # Defining desire outputs vector\n",
    "        if desire_output == 0:       # neuron one's class is activated\n",
    "            main_output = [1, 0, 0]\n",
    "        elif desire_output == 1:     # neuron two's class is activated\n",
    "            main_output = [0, 1, 0]\n",
    "        elif desire_output == 2:     # neuron three's class is activated\n",
    "            main_output = [0, 0, 1]\n",
    "\n",
    "        # Step 1\n",
    "        compare = []\n",
    "        Dcost_Dweight_output = 0\n",
    "        # Dcost_Dweight_output = ( our_output - main_output ) * self.sigmoid_output\n",
    "        for i in range(len(our_output)):\n",
    "            compare.append(our_output[i] - main_output[i])\n",
    "        for i in range(len(compare)):\n",
    "            Dcost_Dweight_output += compare[i] * self.sigmoid_output[i]\n",
    "\n",
    "        # Step 2\n",
    "        Dcost_Dah = [0 for i in range(len(network[1]))]\n",
    "        for neuron in range(len(network[1])):\n",
    "            for j in range(len(compare)):\n",
    "                Dcost_Dah[neuron] += compare[j] * network[1][neuron][j]\n",
    "\n",
    "        # dah_dzh is also a list with 4 values\n",
    "        dah_dzh = self.sigmoid_deriv()\n",
    "        \n",
    "        # dzh_dwh is also a list with 4 values\n",
    "        dzh_dwh = input_of_epoch\n",
    "        \n",
    "        # Calculating the multiply of each list\n",
    "        Dcost_Dweight_hidden = [0 for i in range(len(dah_dzh))]\n",
    "\n",
    "        for i in range(len(dah_dzh)):\n",
    "            Dcost_Dweight_hidden[i] += Dcost_Dah[i] * dah_dzh[i] * dzh_dwh[i]\n",
    "        \n",
    "        # Step 3\n",
    "        # Now we're gonna calculate the new weights\n",
    "        for layer in range(len(network)):\n",
    "            for neuron in range(len(network[layer])):\n",
    "                for weight in range(len(network[layer][neuron])):\n",
    "                    if layer == 0:\n",
    "                        network[layer][neuron][weight] = network[layer][neuron][weight] - (self.alpha * Dcost_Dweight_hidden[weight])\n",
    "                    elif layer == 1:\n",
    "                        network[layer][neuron][weight] = network[layer][neuron][weight] - (self.alpha * Dcost_Dweight_output)\n",
    "    \n",
    "    def train(self, inputs, desire_output, epochs):\n",
    "        \"\"\" inputs is our  input data,\n",
    "            desire_output is our target,\n",
    "            epochs is the number of iteration \n",
    "            What we must do in training is:\n",
    "            step 1: we go forward by using the forward method \n",
    "            step 2: we then go backward by using the backward method,\n",
    "            Step: And other\n",
    "            \"\"\"\n",
    "            \n",
    "        network = self.initialize_networks()\n",
    "        self.counter = 0 # This counter is gonna determine the input and output\n",
    "        \n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            print(\"We're in Epoch \", epoch)\n",
    "            self.counter = 0\n",
    "            for i in range(150):\n",
    "                \n",
    "                # step 1\n",
    "                our_output = self.forward(inputs[self.counter], network)\n",
    "\n",
    "                # step 2\n",
    "                self.backward(inputs[self.counter], desire_output[self.counter], our_output, network)\n",
    "                \n",
    "                # step 4\n",
    "                loss = self.cross_entropy(desire_output[self.counter], our_output)\n",
    "\n",
    "                if desire_output[self.counter] == 0:       # neuron one's class is activated\n",
    "                    main_output = [1, 0, 0]\n",
    "                elif desire_output[self.counter] == 1:     # neuron two's class is activated\n",
    "                    main_output = [0, 1, 0]\n",
    "                elif desire_output[self.counter] == 2:     # neuron three's class is activated\n",
    "                    main_output = [0, 0, 1]\n",
    "\n",
    "                max = -100000\n",
    "                index = 0\n",
    "                for i in range(len(our_output)):\n",
    "                    if our_output[i] > max:\n",
    "                        max = our_output[i]\n",
    "                        index = i\n",
    "                out_list_com = [0, 0, 0]\n",
    "                out_list_com[index] = 1\n",
    "\n",
    "            acc = accuracy_score(main_output, out_list_com)\n",
    "            self.hist['loss'] += [loss]\n",
    "            self.hist['acc'] += [acc]\n",
    "            print('Epoch', epoch, 'loss: ', loss, 'acc: ', acc)\n",
    "            self.counter += 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NeuralNetwork Matrix: {}-{}-{}\" .format(self.input_size, self.hidden_size, self.output_size)\n",
    "    \n",
    "# Let's call the class \n",
    "Epoch = int(input(\"Please enter the number of epochs: \"))\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# Let's Load IRIS\n",
    "iris = load_iris()\n",
    "inputs = iris.data\n",
    "desire_output = iris.target\n",
    "\n",
    "#Let's Train\n",
    "mlp.train(inputs, desire_output, 10)\n",
    "\n",
    "#Let's draw\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('loss')\n",
    "plt.plot(mlp.hist['loss'])\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('acc')\n",
    "plt.plot(mlp.hist['acc'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## THE END\n",
    "\"\"\" Written by ALISHOHADAEE FROM ZERO TO Hero :D \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "#### Calling class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95b98928bc14ac393d9b81e58183357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6f038ee53da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesire_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-335aae7e7b35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, desire_output, epochs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# This counter is gonna determine the input and output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We're in Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "iris = load_iris()\n",
    "inputs = iris.data\n",
    "desire_output = iris.target\n",
    "epoch = 10\n",
    "\n",
    "mlp.train(inputs, desire_output, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('loss')\n",
    "plt.plot(mlp.hist['loss'])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('acc')\n",
    "plt.plot(mlp.hist['acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
